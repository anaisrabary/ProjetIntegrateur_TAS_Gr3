{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pistes de Recherche sur le Projet TAS\n",
    "\n",
    "Etudes sur les données fournies par TAS. \n",
    "Pistes :\n",
    "- Chargement des données\n",
    "- Traitement pour l'affichage\n",
    "- Traitement du bruit\n",
    "- Etude autour de la segmentation des images\n",
    "- Classification des images avec un CNN (Keras)\n",
    "- Lien Spark et Keras\n",
    "\n",
    "\n",
    "## 1.Charger les données\n",
    "\n",
    "On charge les données stockées dans des fichiers .npy.\n",
    "\n",
    "Les labels sont sous la forme :\n",
    "``` \n",
    "    [0,0,0,1,0]\n",
    "```\n",
    "C'est donc un array de 5 cases :\n",
    "- 1 : Urban Area\n",
    "- 2 : Agricultural Territory\n",
    "- 3 : Forest\n",
    "- 4 : Wetlands\n",
    "- 5 : Surface with Water\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabel=np.load('INSA_data_images/test_labels_0_10_25.npy')\n",
    "testRGB = np.load('INSA_data_images/test_RGB_0_10_25.npy')\n",
    "trainLabel= np.load('INSA_data_images/train_labels_0_10_25.npy')\n",
    "trainRGB = np.load('INSA_data_images/train_RGB_0_10_25.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of Test label \", testLabel.shape)\n",
    "# 42805 images avec 5 labels possibles\n",
    "print(\"shape of Test RGB \", testRGB.shape)\n",
    "# 42805 images de 32*32\n",
    "print(\"shape of Train Label \",trainLabel.shape)\n",
    "print(\"shape of Train RGB \", trainRGB.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Etude sur l'affichage des images\n",
    "\n",
    "### 2.1 Prétraitement pixels\n",
    "\n",
    "Si on essaie d'afficher les images brutes, elles apparaisse extrèmement sombre. On n'arrive pas a distinguer à l'oeil nu ce que dit le label.  \n",
    "Un traitement est donc nécessaire sur les images pour pouvoir les afficher. \n",
    "\n",
    "Visiblement, les images ne sont ni codées en $[0..255]$ RGB, ni en % (entre 0 et 1) RGB. \n",
    "Nous avons donc fait des tests à la main pour essayer d'avoir une image lisible. \n",
    "\n",
    "2 méthodes ont été retenues :\n",
    "- multiplier les pixels par 10\n",
    "- Récupérer la valeur max de chaque image et diviser les pixels par cette valeur max. On a ainsi un affichage plus égalisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def myImage(line, data):\n",
    "    image= np.zeros([32,32,3])\n",
    "    for i in range(0,31):\n",
    "        for j in range(0,31):\n",
    "            image[i,j] = data[line,i,j]\n",
    "    maxval = image.max()\n",
    "    \n",
    "    for i in range(0,31):\n",
    "        for j in range(0,31):\n",
    "            image[i,j] = image[i,j]/maxval\n",
    "    return image\n",
    "\n",
    " def myImage2(line, data):\n",
    "    image= np.zeros([32,32,3])\n",
    "    for i in range(0,31):\n",
    "        for j in range(0,31):            \n",
    "            image[i,j] = data[line,i,j]*10\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Traitement du bruit (images noires)\n",
    "\n",
    "On se rend compte aussi que certaines images ont tous leur pixel à 0. Elles sont donc toute noires. Cependant elles sont quand même labélisées. \n",
    "On considère que c'est du bruit et nous décidons de filtrer le dataset pour suprimer ces images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_black_image(data) :\n",
    "    indextodelete = []\n",
    "    for i in range(0, data.shape[0]):\n",
    "        if np.count_nonzero(data[i])==0:\n",
    "            indextodelete.append(i)\n",
    "    return indextodelete\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexDeleteTrain= delete_black_image(trainRGB)\n",
    "indexDeleteTest= delete_black_image(testRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nb d'images 100% noires dans Train RGB : \",len(indexDeleteTrain))\n",
    "\n",
    "print(\"Nb d'images 100% noires dans Test RGB : \",len(indexDeleteTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(trainRGB.shape[0],dtype=bool) #np.ones_like(a,dtype=bool)\n",
    "mask[indexDeleteTrain] = False\n",
    "trainRGB_clean= trainRGB[mask]\n",
    "trainlabel_clean= trainLabel[mask]\n",
    "print(\"shape of cleaned Train RGB \", trainRGB_clean.shape)\n",
    "\n",
    "mask = np.ones(testRGB.shape[0],dtype=bool) #np.ones_like(a,dtype=bool)\n",
    "mask[indexDeleteTest] = False\n",
    "testRGB_clean= testRGB[mask]\n",
    "testlabel_clean= testLabel[mask]\n",
    "print(\"shape of cleaned Train RGB \", testRGB_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('INSA_data_images/CLEAN_test_labels_0_10_25.npy', testlabel_clean )\n",
    "np.save('INSA_data_images/CLEAN_test_RGB_0_10_25.npy', testRGB_clean)\n",
    "np.save('INSA_data_images/CLEAN_train_labels_0_10_25.npy', trainlabel_clean)\n",
    "np.save('INSA_data_images/CLEAN_train_RGB_0_10_25.npy', trainRGB_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Affichage des images\n",
    "\n",
    "D'abord des images de tests, puis des images d'entrainement. On affiche aussi les images noires (pour voir leur label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des images de test\n",
    "fig, axarr = plt.subplots(20, sharex=True,  figsize=(100,100))\n",
    "\n",
    "for i in range(0,20):\n",
    "    im=myImage(i,testRGB)\n",
    "    axarr[i].imshow(im)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage pour les images Black\n",
    "fig, axarr = plt.subplots(10, sharex=True,  figsize=(20,20))\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    im=myImage(indexDelete[i],trainRGB)\n",
    "    axarr[i].imshow(im)\n",
    "    if (trainLabel[indexDelete[i],0]==1.):\n",
    "        axarr[i].set_title(\"Urban Area\")\n",
    "    if (trainLabel[indexDelete[i],1]==1.):\n",
    "        axarr[i].set_title(\"Agricultural territory\")\n",
    "    if (trainLabel[indexDelete[i],2]==1.):\n",
    "        axarr[i].set_title(\"Forest\")\n",
    "    if (trainLabel[indexDelete[i],3]==1.):\n",
    "        axarr[i].set_title(\"wetlands\")\n",
    "    if (trainLabel[indexDelete[i],4]==1.):\n",
    "        axarr[i].set_title(\"Surface with water\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des images de train avec leur label\n",
    "fig, axarr = plt.subplots(40, sharex=True,  figsize=(300,300))\n",
    "\n",
    "\n",
    "for i in range(0,40):\n",
    "    im=myImage(i,trainRGB)\n",
    "    axarr[i].imshow(im)\n",
    "    if (trainLabel[i,0]==1.):\n",
    "        axarr[i].set_title(\"Urban Area\")\n",
    "    if (trainLabel[i,1]==1.):\n",
    "        axarr[i].set_title(\"Agricultural territory\")\n",
    "    if (trainLabel[i,2]==1.):\n",
    "        axarr[i].set_title(\"Forest\")\n",
    "    if (trainLabel[i,3]==1.):\n",
    "        axarr[i].set_title(\"wetlands\")\n",
    "    if (trainLabel[i,4]==1.):\n",
    "        axarr[i].set_title(\"Surface with water\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recherche pour la partie apprentissage \n",
    "\n",
    "### 3.1 Segmentation\n",
    "\n",
    "Dispo avec Scikit Learn, méthode non supervisée de Felzenszwalb.\n",
    "\n",
    "On a lu beaucoup de litérature sur la segmentation d'image mais nous n'avons en fait pas les données nécessaires pour pouvoir faire un entrainement et de la segmentation d'image car nos images d'entrainement n'ont pas de calques.    \n",
    "Effectivement, nos images ont un label valabe pour la totalité de l'image et non pas seulement une partie de cette image.  \n",
    "\n",
    "On obtient des contours (en jaune) sur l'image. Nos recherches n'ont pas été poursuivies plus loin, car nous avons recentré l'étude sur la classification des images et non pas la segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bfaba06bc118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msobel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfelzenszwalb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.segmentation import felzenszwalb\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(20, sharex=True,  figsize=(100,100))\n",
    "\n",
    "for i in range (0,20):\n",
    "    img = myImage(i, trainRGB)\n",
    "    segments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)\n",
    "    #print(\"Felzenszwalb number of segments: {}\".format(len(np.unique(segments_fz))))\n",
    "    ax[i].imshow(mark_boundaries(img, segments_fz))\n",
    "    if (trainLabel[i,0]==1.):\n",
    "        ax[i].set_title(\"Urban Area\")\n",
    "    if (trainLabel[i,1]==1.):\n",
    "        ax[i].set_title(\"Agricultural territory\")\n",
    "    if (trainLabel[i,2]==1.):\n",
    "        ax[i].set_title(\"Forest\")\n",
    "    if (trainLabel[i,3]==1.):\n",
    "        ax[i].set_title(\"wetlands\")\n",
    "    if (trainLabel[i,4]==1.):\n",
    "        ax[i].set_title(\"Surface with water\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Etude de CNN\n",
    "\n",
    "D'après nos études comparatives de mméthodes d'apprentissage vues en TP, nous avons vu que les réseaux de neurones sont plus performants que KNN en terme de temps d'exécution. Une fois le réseau entrainé, la classification est quasi-instantanées.  \n",
    "Les réseaux de neuronnes sont donc plus adaptés à notre scénario de cas d'utilisation pour ce projet. \n",
    "\n",
    "Afin d'obtenir un modèle de réseau de neurones \"sur mesure\" qui corresponde aux données que l'on possède, on a choisi de se concentrer sur les CNN, Convolutional Neural Network. \n",
    "\n",
    "Ci-après est présenté et décris un CNN fait à la main. Il n'est pas extrèmement performant mais permet de décrire et comprendre un cas d'étude complet.\n",
    "\n",
    "Par la suite, on utilisera des CNN plus conséquants et dont l'architecture est reconnue pour des problèmes de classification (comme AlexNet pour les problèmes de Fetch-Decode). Ces méthodes seront étudiées dans un autre document en annexe.   \n",
    "\n",
    "L'outils de prédilection pour aborder CNN est Keras, qui repose sur un Backend TensorFlow. Il a l'avantage d'être facile à comprendre pour débuter une étude. D'autre par, si les modèles se complexifie il est possible de continuer avec toutes les possibilités qu'offre Tensorflow.\n",
    "\n",
    "WORKFLOW de KERAS : \n",
    "1. Training Data \n",
    "2. Create Model (keras.layers())\n",
    "3. Configure model (model.compile())\n",
    "4. Train model (model.fit())\n",
    "5. On trained Model, inject testing data then A OR B\n",
    "  A. Evaluate model (model.evaluate()) => loss\n",
    "  B. get predictions (model.predict()) => prediction\n",
    "\n",
    " Pour la partie compile, il faut configurer :\n",
    " - Spécifier un Optimizer qui determine comment les poids sont mis à jour\n",
    " - Spécifier le type de la cost function or loss function\n",
    " - Spécifier la métrique a évaluer pendant le training et testing\n",
    " - Creer le graphe du model en utilisant le backend\n",
    " - ....\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modèle basique\n",
    "\n",
    "Cette modèle est conçu manuellement (non basé sur une architecture concrète).\n",
    "\n",
    "#### a. Présentation générale\n",
    "\n",
    "Notre modèle est composé de 7 couches cachées (hidden layers). Grâce à sa simplicité, on peut décrire la fonctionnalité des couches.\n",
    "\n",
    "L'avantage d'utiliser un CNN plutôt qu'un simple Multilayer Perceptron est qu'il nous permet de ne plus devoir centrer nos images sur l'objet recherché. Le CNN devient \"translation-invariant\", invariant selon les translations. \n",
    "\n",
    "D'autre part, le multilayered perceptron network est \"fully connected\", cela veut dire que tous les perceptrons d'une couche sont connectés à tous les perceptrons de la couche suivante, et ainsi de suite. Donc il devient très compliqué de faire un grand réseau profond. \n",
    "CNN permet de choisir comment relier les couches entre elles et ainsi diminuer le nombre de paramètres à définir.\n",
    "\n",
    "#### b. CNN\n",
    "CNN est un réseau de neurones dit \"feedforward\", nourri en avant.  \n",
    "\n",
    "Pour commencer, on peut séparer les couches en 2 groupes :\n",
    "- les couches 1 à 6 permettent d'extraire les features de l'image\n",
    "- la couche 7 permet de classifier l'image étudiée.\n",
    "\n",
    "Le 1er groupe est constitué de couches convolutionnelles, de courches denses et  de couches de max-pooling qui de trouver les features.\n",
    "\n",
    "La couche 7, elle, est 100% connectée afin d'agir comme un classifier. Si sa fonction d'activation est \"softmax\", elle nous donne le label de l'image. Si c'est \"sigmoid\", elle nous donne la probabilité que l'image soit ce label.\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 10px\">\n",
    "    <div class=\"col-md-offset-3 col-md-6\">\n",
    "        <img src=\"img_notebook/cnn-schema1.jpg\" style=\"margin-right: 0; width: 3500px;\" />\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "**Les courches convolutionnelle**\n",
    "C'est elles qui permettent de détecter des critères spécifiques. Si elles trouvent ce qu'elles cherchent, elles ont une forte réponse.\n",
    "C'est en fait un filtre convolutionnel qui se balade sur la totalité de l'image et qui fait une somme pondérée pour caque localisation.\n",
    "\n",
    "Dans notre cas pour la 1ere couche convolutionnelle, on a une entrée de $32*32*3$. Notre couche doit obligatoirement être de profondeur 3, comme notre array en entrée. \n",
    "Ensuite pour choisir la largeur et la hauteur du filtre, il faut s'assurer que celui ci reste dans l'image (on ne peut pas avoir un filtre qui soit a moitié en dehors de l'image). Pour cela, on peut soit retirer des pixels sur les côtés, soit ajouter du pading à l'image.\n",
    "\n",
    "La sortie de l'image passée au filtre convolutionnel de la couche est appelé **activation map**. Elle est de profondeur 1.\n",
    "\n",
    "Exemple : si on a une image de $32*32*3$ et un filtre de taille $3*3*3$, l'activation map est de taille $30*30*1$. il y a 2 colones et 2 lignes de pixels coupés pour que le filtre puisse se balader uniquement dans l'image. L'activation map est donc une couche de $30*30=900$ neurones.\n",
    "\n",
    "Il est aussi possible d'utiliser **plusieurs filtres**.  C'est le cas de notre 1ere couche convolutionnelle qui est de taille $30*30*32$. Et cest 32 filtres partages les même poids, donc on a $3*3*3*32 = 288$ poids.\n",
    "En fait, pour chaque filtre, on déplace l'image d'un pixel. donc si on a 32 filtres, on a déplacé l'image 32 fois.\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 10px\">\n",
    "    <div class=\"col-md-offset-3 col-md-6\">\n",
    "        <img src=\"img_notebook/activation-maps-32-kernel.jpg\" style=\"margin-right: 0; width: 3500px;\" />\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchiration des caractéristiques** (des features) :\n",
    "\n",
    "CNN, a travers ses couches succesives permer de hierarchiser les features. On arrive a trouver des features par effet de cascade : On part d'une image, on observe une portion de cette image. Puis dans cette portion, on observe un eplus petite portion. Et c'est dans cette dernière que l'on trouve la feature. (C'est un peu Inception).\n",
    "\n",
    "**Les couches de Max Pooling** :  \n",
    "On a presque toujours une couche max pooling après une couche convolutionnelle.  \n",
    "Le Pooling nous permet de réduire la taille de l'espace (uniquement la largeur et hauteur, pas la profondeur). On peut ainsi réduire le nombre de paramètres et éviter l'overfitting. (l'overfitting s'exprime lorsque notre modèle est \"trop bon\", il répond parfaitement aux données d'entrainement mais n'est pas robustes aux données de test).\n",
    "\n",
    "Max pooling est la forme de pooling la plus utilisée : on applique un filtre de taille p et on récupère la valeur max de cette partie de l'image.\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 10px\">\n",
    "    <div class=\"col-md-offset-3 col-md-6\">\n",
    "        <img src=\"img_notebook/max-pooling-demo.jpg\" style=\"margin-right: 0; width: 3500px;\" />\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "\n",
    "def createModel(inputshape,nClasses):\n",
    "# empilement de Conv layers puis de Max pooling layers.\n",
    "# Dropout permet d'éviter l'overfitting\n",
    "# A la fin, on a une fully connected layer (Dense) suivie d'une sopftmax layer\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same',activation='relu', input_shape=inputshape))\n",
    "    # 32 filters/kernels with (3*3) size window.\n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25)) # 0.25 : dropout ratio\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nClasses, activation='sigmoid'))\n",
    "    #model.add(Dense(nClasses, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 6 Conv layze, 1 fully-connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 273,573\n",
      "Trainable params: 273,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputshape =(32,32,3)\n",
    "nclasses = 5\n",
    "mod = createModel(inputshape, nclasses)\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Expérimentation\n",
    "\n",
    "**Compiler le modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "mod.compile(optimizer=opt, loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement des données \n",
    "# DEPRECEATE (no need to trained model on prepared data)\n",
    "\n",
    "for i in range (1, 43000):\n",
    "    if i==100 or i==1000 or i==10000 or i==30000:\n",
    "        print(i)\n",
    "    trainRGB[i]=(myImage2(i, trainRGB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random data :\n",
    "#indices = np.random.randint(1000, size=1000)\n",
    "\n",
    "# DONNEES NON TRAITEES\n",
    "#data = trainRGB[indices]\n",
    "#target = trainLabel[indices]\n",
    "data = trainRGB_clean\n",
    "target = trainlabel_clean\n",
    "xtrain = data\n",
    "ytrain = target\n",
    "xtest = testRGB_clean\n",
    "ytest = testLabel_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrainer le modèle**  \n",
    "\n",
    "Pour entrainer le modèle, il faut faire plusieurs choix :\n",
    "- la taille de batch\n",
    "- le nombre d'époche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163989 samples, validate on 42805 samples\n",
      "Epoch 1/1\n",
      "163989/163989 [==============================] - 1103s 7ms/step - loss: 0.7278 - acc: 0.7155 - val_loss: 0.7216 - val_acc: 0.7237\n"
     ]
    }
   ],
   "source": [
    "history = mod.fit(xtrain,ytrain, batch_size=50, epochs=1, verbose=1,validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation du modèle**  \n",
    "C'est optionnel si on a passé les données de validation du modèle dans le fit comme ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42805/42805 [==============================] - 93s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7215780501387384, 0.7237238640429302]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.evaluate(xtest,ytest, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**  \n",
    "On veut prédire le label de nos images de tests. \n",
    "Dans l'exemple ci-après, on prédit pour les images 101 à 110 du test RGB. Le modèle nous donne en pourcentage la classification de l'image pour tel ou tel label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred =mod.predict(testRGB_clean[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction  [0.12229992 0.6103879  0.09480256 0.01460406 0.00769571]\n",
      "label       [0. 1. 0. 0. 0.]\n",
      "prediction  [0.2536406  0.40241262 0.35615438 0.24223669 0.14956376]\n",
      "label       [0. 1. 0. 0. 0.]\n",
      "prediction  [0.03598148 0.06905532 0.104638   0.20048258 0.2525326 ]\n",
      "label       [0. 0. 0. 0. 1.]\n",
      "prediction  [0.7078749  0.16889891 0.05113503 0.03536195 0.011864  ]\n",
      "label       [1. 0. 0. 0. 0.]\n",
      "prediction  [0.1905594  0.1298222  0.09139905 0.2150234  0.25065726]\n",
      "label       [0. 0. 0. 1. 0.]\n",
      "prediction  [0.22913529 0.53453684 0.09326366 0.01978889 0.00995544]\n",
      "label       [0. 1. 0. 0. 0.]\n",
      "prediction  [0.53851706 0.31449786 0.05257269 0.01578066 0.00531275]\n",
      "label       [0. 1. 0. 0. 0.]\n",
      "prediction  [0.02355546 0.07582994 0.20053986 0.21445827 0.15407501]\n",
      "label       [0. 1. 0. 0. 0.]\n",
      "prediction  [0.13049115 0.5958856  0.17758335 0.03681044 0.01986633]\n",
      "label       [0. 0. 1. 0. 0.]\n",
      "prediction  [0.6146936  0.18203664 0.04271176 0.02926459 0.0102383 ]\n",
      "label       [0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "lab = testlabel_clean[20:30]\n",
    "for i in range(0, len(pred)):\n",
    "    print(\"prediction \", pred[i])\n",
    "    print(\"label      \", lab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Sauvegarder le modèle\n",
    "Il peut être utile de réutiliser un modèle sans avoir à le réentrainer. Pour cela, il faut sauvegarder le modèle dans un fichier.  \n",
    "\n",
    "**sauvegarder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json \n",
    "model_json = mod.to_json()\n",
    "with open(\"Keras_Model_Trained/modelTrained.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "mod.save_weights(\"Keras_Model_Trained/modelTrained.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recharger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json \n",
    "jsonfile = open('Keras_Model_Trained/modelTrained.json', 'r')\n",
    "loaded_model_json = jsonfile.read()\n",
    "jsonfile.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"Keras_Model_Trained/modelTrained.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Modèle profonde\n",
    "\n",
    "Ce modèle est basé sur une architecture existante (il s'agit de **DenseNet**), elle est plus profonde. Nous utilisons le technique appellé **Transfer Learning** pour adapter à notre cas d'utilisation.\n",
    "\n",
    "#### a. DenseNet.\n",
    "Bien que, plusieurs architectures sont proposés par plusieurs laboratoires / entreprises lors des challenges Open data (for example ImageNet). Dans le cas de problème, pour cause de contraint de temps de développement, nous étudions seulement les architectures \"classique\" déjà disponibles sur Keras: **Xception**, **VGG**, **ResNet**, etc. (Cette étude est détaillé dans rapport). \n",
    "\n",
    "Nous choisissons **DenseNet** pour le compromise entre précision et le temps d'apprentissage, plus précisément il s'agit le modèle **DenseNet121**. \n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 10px\">\n",
    "    <div class=\"col-md-12\">\n",
    "        <img src=\"img_notebook/DenseNet.png\" style=\"margin-right: 0; width: 100%\" />\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Résultats\n",
    "\n",
    "#### a. Sur des données non traitées\n",
    "**1er test :**\n",
    "```\n",
    "Train on 4000 samples, validate on 1000 samples\n",
    "Epoch 1/1\n",
    "4000/4000 [==============================] - 25s 6ms/step - loss: 1.3918 - acc: 0.3845 - val_loss: 1.3303 - val_acc: 0.3600\n",
    "```\n",
    "```\n",
    "Train on 33600 samples, validate on 8400 samples\n",
    "Epoch 1/1\n",
    "33600/33600 [==============================] - 248s 7ms/step - loss: 0.9733 - acc: 0.6020 - val_loss: 0.7275 - val_acc: 0.6975\n",
    "```\n",
    "```\n",
    "Train on 171000 samples, validate on 42805 samples\n",
    "Epoch 1/1\n",
    "170950/171000 [============================>.] - ETA: 0s - loss: 0.7488 - acc: 0.7100\n",
    "```\n",
    "\n",
    "On constate ici que plus on a de données à apprendre, meilleure est la classification.\n",
    "\n",
    "**2eme tests sur les données filtrées :**\n",
    "ici on a entrainé et testé le modèle après avoir enlevé toutes les images noires. On a une accuracy meilleure de 1,3 points par rapport au dernier résultat ci-dessus.\n",
    "```\n",
    "Train on 163989 samples, validate on 42805 samples\n",
    "Epoch 1/1\n",
    "163989/163989 [==============================] - 1103s 7ms/step - loss: 0.7278 - acc: 0.7155 - val_loss: 0.7216 - val_acc: 0.7237\n",
    "```\n",
    "\n",
    "#### b. Sur des données traitées\n",
    "**myImage** :\n",
    "Avec myImage, le traitement est fait en récupérant la valeur max des pixels et en divisant les autres pixels par cette valeur max. On égalise ainsi les couleurs de l'image pour l'oeil humain. \n",
    "\n",
    "Cependant, on se rend compte que le modèle ne sais pas du tout comment lire ces données. Le score est extrèmement faible, un peu moins de 9%.\n",
    "```\n",
    "Train on 33600 samples, validate on 8400 samples\n",
    "Epoch 1/1\n",
    "33600/33600 [==============================] - 342s 10ms/step - loss: nan - acc: 0.0882 - val_loss: nan - val_acc: 0.0888\n",
    "```\n",
    "**myImage2**:\n",
    "Avec ce traitement myImage2, on fait simplement que multipuler par 10 tous les pixels de l'image. \n",
    "On retrouve ici le score atteint avec des données non traitées. \n",
    "\n",
    "Il n'est donc pas pertinent pour le modèle d'être entrainé sur des données pré-traitées. Le traitement reste pertinent pour l'affichage \"humain\".\n",
    "```\n",
    "Train on 33600 samples, validate on 8400 samples\n",
    "Epoch 1/1\n",
    "33600/33600 [==============================] - 256s 8ms/step - loss: 0.9152 - acc: 0.6212 - val_loss: 0.7036 - val_acc: 0.7150\n",
    "```\n",
    "\n",
    "### 3.5 Pistes à améliorer :\n",
    "\n",
    "Nous avons fait une validation de notre modèle assez basique. Il faudrait faire du k-fold validation et si possible du n-fold validation. Ainsi on serait assurée de la robustesse du modèle.\n",
    "\n",
    "D'autre part, il faudrait pousser les recherches sut l'architecture du réseau en lui même pour que le modèle puisse avoir un meilleur score. c'est une tâche très chronophage que nous avons choisi d'écarter du projet afin de pouvoir fournit un projet intégré dans sa totalité et non pas centralisé sur la partie traitement des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partie avec Spark\n",
    "L'objectif de ce projet étant d'avoir une architecture et un pipeline complet et pouvant être distribué, il faut aussi prévoir la distribution de la partie apprentissage et/ou prediction des données.\n",
    "\n",
    "Pour cela, Spark est idéal car il nous permet de faire des calculs distribués sur d'autres machines et également faire du calcul sur GPU.   \n",
    "\n",
    "Il existe des outils pour nous aider à combiner kéras et Spark. Nous avons choisi ***Elephas*** qui permet d'avoir pyspark et keras.   \n",
    "\n",
    "Tout d'abord il faut lancer la configuration de spark pour Elephas. Puis on peut directement récupérer le modèle fait avec Keras. Il est également possible de donner à spark un modèle déjà pré-entrainé. Dans ce cas, il ne faut pas oublier de recompiler le modèle keras avant de le donner à Spark.  \n",
    "\n",
    "Si l'on veut entrainer notre modèle sur spark, il faut mettre les données xtrain et ytrain dans un RDD. Elephas nous permet de préparer cette structure de données.  \n",
    "\n",
    "Un modèle basique dans Elephase est un SparkModel. On l'initialise en lui passant une fréquence de mise à jour et un mode de parallelization dans un modèle keras compilé.\n",
    "Après on fait juste le traditionnel *fit* sur les données RDD (même options que Keras, donc on peut lui donner un batch_size et un epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('Elephas_App').setMaster('local[8]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "rdd = to_simple_rdd(sc, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on a un exemple d'entrainement de modele sur spark. Exécuté dans une petite VM, j'ai juste voulu testé que cela marchait. J'ai entrainé le modèle avce seulement 100 données. C'est pour cela que le score est seulement de  5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "spark_model = SparkModel(mod, frequency='epoch', mode='synchronous')\n",
    "spark_model.fit(rdd, batch_size=32, epochs=10, verbose=0,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42805/42805 [==============================] - 74s 2ms/step\n",
      "accuracy  [1.6097211157101752, 0.05018105361523187]\n"
     ]
    }
   ],
   "source": [
    "score =spark_model.master_network.evaluate(xtest, ytest, verbose=1)\n",
    "print('accuracy ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on veut donner à spark un modèle pré-entrainer, il faut charger le modèle (cf partie 3.4). En évaluant ce modèle, on le même taux de réussite que lorsque l'on ne passait pas par spark. Spark a pu récupéré le modèle pré-entrainé par Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42805/42805 [==============================] - 64s 1ms/step\n",
      "accuracy  [0.7215780546252742, 0.7237238640429302]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "loaded_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "sparkmod = SparkModel(loaded_model, frequency='epoch',mode='synchronous')\n",
    "scoreT=sparkmod.master_network.evaluate(xtest, ytest, verbose=1)\n",
    "print('accuracy ', scoreT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
